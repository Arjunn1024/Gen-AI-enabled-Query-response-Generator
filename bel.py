# -*- coding: utf-8 -*-
"""BEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZYlvbEoI1Uhi2qWpC--XPvO8OX7wvfa3
"""

pip install PyMuPDF transformers torch openai datasets

pip install transformers[torch] accelerate -U

import fitz  # PyMuPDF
import re
import pandas as pd
from transformers import BertTokenizer, BertModel
import torch
import openai
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from torch.utils.data import Dataset
from datasets import load_metric

# Step 1: Extract and Preprocess Text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    document = fitz.open(pdf_path)
    for page_num in range(document.page_count):
        page = document.load_page(page_num)
        text += page.get_text()
    return text

def preprocess_text(text):
    # Remove unnecessary newline characters
    text = re.sub(r'\n+', ' ', text)  # Replace multiple newlines with a space
    paragraphs = text.split('. ')  # Split text into sentences
    cleaned_paragraphs = [para.strip() + '.' for para in paragraphs if para.strip()]
    return cleaned_paragraphs

pdf_path = "/content/Netaji speech.pdf"#can put any pdf
book_text = extract_text_from_pdf(pdf_path)
cleaned_paragraphs = preprocess_text(book_text)

print(cleaned_paragraphs)

print(f"Number of paragraphs extracted: {len(cleaned_paragraphs)}")
print("Sample paragraphs:")
for i, paragraph in enumerate(cleaned_paragraphs[:50]):
    print(f"Paragraph {i+1}: {paragraph}")

# Step 2: Understand and Encode Text using BERT
bert_model_name = "bert-base-uncased"
bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)
bert_model = BertModel.from_pretrained(bert_model_name)

def encode_paragraphs(paragraphs, tokenizer, model):
    encoded_paragraphs = []
    for paragraph in paragraphs:
        # Tokenize the paragraph
        inputs = tokenizer(paragraph, return_tensors='pt', truncation=True, padding=True, max_length=512)
        # Pass the tokens through the BERT model
        with torch.no_grad():
            outputs = model(**inputs)
        # Extract the hidden states from the outputs
        hidden_states = outputs.last_hidden_state
        # Take the mean of the hidden states to get a single vector representation for the paragraph
        paragraph_embedding = hidden_states.mean(dim=1)
        # Append the embedding to the list
        encoded_paragraphs.append(paragraph_embedding.squeeze().numpy())
    return encoded_paragraphs

encoded_paragraphs = encode_paragraphs(cleaned_paragraphs, bert_tokenizer, bert_model)

# encoded_paragraphs = encode_paragraphs(cleaned_paragraphs)

# Step 3: Generate Initial Questions Using GPT-3
openai.api_key = 'your_api_key'

import time
def generate_questions(paragraph, max_retries=5, wait_time=2):
    retries = 0
    while retries < max_retries:
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Generate five meaningful questions based on the following paragraph:\n\n{paragraph}\n\nQuestions:"}
                ],
                max_tokens=150,
                n=1,
                stop=None
            )
            questions = response['choices'][0]['message']['content'].strip().split('\n')
            return questions
        except openai.error.RateLimitError:
            retries += 1
            time.sleep(wait_time)
            wait_time *= 2
        except openai.error.APIError as e:
            print(f"API Error: {e}")
            return []
        except openai.error.APIConnectionError as e:
            print(f"API Connection Error: {e}")
            retries += 1
            time.sleep(wait_time)
            wait_time *= 2
        except openai.error.InvalidRequestError as e:
            print(f"Invalid Request: {e}")
            return []
        except openai.error.AuthenticationError as e:
            print(f"Authentication Error: {e}")
            return []
        except openai.error.PermissionError as e:
            print(f"Permission Error: {e}")
            return []
        except Exception as e:
            print(f"Unexpected Error: {e}")
            return []
    return []

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk import download
from nltk.tag import pos_tag
import nltk
nltk.download('averaged_perceptron_tagger')
download('punkt')
def generate_question(paragraph):
    sentences = sent_tokenize(paragraph)
    questions = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        pos_tags = pos_tag(words)

        # Basic heuristic to identify potential question subjects (nouns, pronouns)
        subject = None
        verb = None
        for word, tag in pos_tags:
            if tag in ['NN', 'NNS', 'NNP', 'NNPS']:  # Nouns
                subject = word
            elif tag.startswith('VB'):  # Verbs
                verb = word
            if subject and verb:
                break

        if subject and verb:
            question = f"What {verb} {subject}?"
            questions.append(question)

    return questions

import concurrent.futures
paragraphs_with_questions = []
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    future_to_paragraph = {executor.submit(generate_question, paragraph): paragraph for paragraph in cleaned_paragraphs}
    for future in concurrent.futures.as_completed(future_to_paragraph):
        paragraph = future_to_paragraph[future]
        try:
            questions = future.result()
            paragraphs_with_questions.append((paragraph, questions))
        except Exception as exc:
            print(f"Generated an exception: {exc}")
            paragraphs_with_questions.append((paragraph, []))

# Optional: Save the results to a DataFrame for further analysis
df = pd.DataFrame(paragraphs_with_questions, columns=['Paragraph', 'Questions'])
df.to_csv('paragraphs_with_questions.csv', index=False)

print("Sample entries from paragraphs_with_questions:")
for i, (paragraph, questions) in enumerate(paragraphs_with_questions):
    print(f"Paragraph {i+1}: {paragraph}")
    print(f"Questions: {questions}")

print(paragraphs_with_questions)

# Optional: Save the results to a DataFrame for further analysis
df = pd.DataFrame(paragraphs_with_questions, columns=['Paragraph', 'Questions'])
df.to_csv('paragraphs_with_questions.csv', index=False)

data = {'paragraph': [], 'question': []}

for paragraph, questions in paragraphs_with_questions:
    for question in questions:
        if question.strip():
            data['paragraph'].append(paragraph)
            data['question'].append(question.strip())

df = pd.DataFrame(data)
df_train = df.sample(frac=0.8, random_state=42)
df_val = df.drop(df_train.index)

print("Length of df:", len(df))
print("Length of df_train:", len(df_train))
print("Length of df_val:", len(df_val))

# Step 4: Fine-Tune T5
model_name = "t5-small"
t5_tokenizer = T5Tokenizer.from_pretrained(model_name)
t5_model = T5ForConditionalGeneration.from_pretrained(model_name)

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from torch.utils.data import Dataset

# Define the dataset class
class ParagraphQuestionDataset(Dataset):
    def __init__(self, dataframe, tokenizer):
        self.dataframe = dataframe
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        inputs = self.tokenizer(row['paragraph'], max_length=512, truncation=True, padding='max_length', return_tensors='pt')
        targets = self.tokenizer(row['question'], max_length=64, truncation=True, padding='max_length', return_tensors='pt')
        inputs["labels"] = targets["input_ids"]
        inputs = {k: v.squeeze() for k, v in inputs.items()}
        return inputs


# Initialize the tokenizer and model
t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')

# Create the datasets
train_dataset = ParagraphQuestionDataset(df_train, t5_tokenizer)
val_dataset = ParagraphQuestionDataset(df_val, t5_tokenizer)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Create the Trainer instance
trainer = Trainer(
    model=t5_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Start training
trainer.train()

def generate_questions_t5(paragraph):
    input_text = f"generate question: {paragraph}"
    input_ids = t5_tokenizer(input_text, return_tensors="pt").input_ids
    outputs = t5_model.generate(input_ids)
    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return question

generated_questions = []
for paragraph in df_val['paragraph']:
    question = generate_questions_t5(paragraph)
    generated_questions.append(question)

df_val['generated_question'] = generated_questions

pip install rouge_score

bleu_metric = load_metric('bleu')
rouge_metric = load_metric('rouge')

def tokenize_text(text, tokenizer):
    return tokenizer.tokenize(text)

# Prepare the data
references = [[tokenize_text(ref, t5_tokenizer)] for ref in df_val['question']]
predictions = [tokenize_text(pred, t5_tokenizer) for pred in df_val['generated_question']]

# Compute BLEU
bleu_score = bleu_metric.compute(predictions=predictions, references=references)

# Compute ROUGE
rouge_score = rouge_metric.compute(predictions=predictions, references=references)

print("BLEU Score:", bleu_score)
print("ROUGE Score:", rouge_score)